<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AR Shoe Try-On</title>
    
    <!-- Mission 6: Load Three.js (the 3D library) -->
    <script async src="https://unpkg.com/es-module-shims@1.6.3/dist/es-module-shims.js"></script>
    <script type="importmap">
    {
      "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.160.0/build/three.module.js",
        "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.160.0/examples/jsm/"
      }
    }
    </script>

    <style>
        body {
            margin: 0;
            padding: 0;
            overflow: hidden;
            background-color: #000;
        }

        /* Mission 4: The camera feed */
        #webcam {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            object-fit: cover;
            transform: scaleX(-1); /* Flip horizontally (mirror) */
        }

        /* Mission 6: The 3D shoe canvas */
        #three_canvas {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            pointer-events: none; /* Make it see-through for clicks */
            transform: scaleX(-1); /* Flip to match the video */
        }

        /* Mission 5: The 2D landmark canvas (we will hide this now) */
        #output_canvas {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            pointer-events: none;
            transform: scaleX(-1);
            /* NEW: Hide the 2D dots now that the 3D shoe works */
            display: none; 
        }

        #loading_message {
            position: fixed;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            color: white;
            font-family: sans-serif;
            font-size: 1.2rem;
            text-align: center;
            width: 90%; /* Ensure it doesn't get too wide */
        }
    </style>
</head>

<body>
    <!-- Layer 1: The Camera Feed -->
    <video id="webcam" autoplay playsinline muted></video>

    <!-- Layer 2: The 3D Shoe Scene -->
    <canvas id="three_canvas"></canvas>

    <!-- Layer 3: The 2D Landmark Dots (now hidden) -->
    <canvas id="output_canvas"></canvas>

    <!-- Loading Message -->
    <div id="loading_message">
        <!-- UPDATED: Added an ID to the text for more detailed messages -->
        <p id="loading_text">Loading AR Model...</p>
    </div>

    <!-- 
      This is the main "Brain" for the entire app.
      It combines all missions into one script.
    -->
    <script type="module">
        // --- MISSION 6: THREE.JS IMPORTS ---
        import * as THREE from 'three';
        import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';

        // --- MISSION 5: MEDIAPIPE IMPORTS ---
        import {
            PoseLandmarker,
            FilesetResolver,
            DrawingUtils
        } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm/vision_bundle.js";

        // --- Get HTML Elements ---
        const videoElement = document.getElementById('webcam');
        const canvasElement = document.getElementById("output_canvas");
        const canvasCtx = canvasElement.getContext("2d");
        const loadingMessage = document.getElementById("loading_message");
        const threeCanvas = document.getElementById("three_canvas");
        // UPDATED: Get the loading text element
        const loadingText = document.getElementById("loading_text");

        // --- MediaPipe (Mission 5) Variables ---
        let poseLandmarker;
        let runningMode = "VIDEO";
        let webcamRunning = false;
        const drawingUtils = new DrawingUtils(canvasCtx);
        let lastVideoTime = -1;
        
        // --- Three.js (Mission 6) Variables ---
        let scene, camera, renderer, shoeModel;
        const clock = new THREE.Clock();

        // --- NEW - Mission 7: Foot Landmark IDs ---
        // These are the specific points we want to track
        const LANDMARK_IDS = {
            LEFT_HEEL: 29,
            LEFT_FOOT_INDEX: 31,
            RIGHT_HEEL: 30,
            RIGHT_FOOT_INDEX: 32
        };

        // ======================================================
        //  FUNCTIONS
        // ======================================================

        // --- Mission 6: Setup 3D Scene ---
        function setupThreeJS() {
            console.log("Setting up 3D scene...");
            scene = new THREE.Scene();

            camera = new THREE.PerspectiveCamera(
                75,
                window.innerWidth / window.innerHeight,
                0.1,
                1000
            );
            camera.position.z = 5; // Start camera 5 units back

            renderer = new THREE.WebGLRenderer({
                canvas: threeCanvas,
                alpha: true // CRITICAL: Make it transparent
            });
            renderer.setSize(window.innerWidth, window.innerHeight);
            renderer.setPixelRatio(window.devicePixelRatio);

            const ambientLight = new THREE.AmbientLight(0xffffff, 1.2);
            scene.add(ambientLight);
            const directionalLight = new THREE.DirectionalLight(0xffffff, 1);
            directionalLight.position.set(0, 1, 2);
            scene.add(directionalLight);

            // This function will now update the loading text
            loadShoeModel();
        }

        // --- Mission 6: Load 3D Shoe Model ---
        function loadShoeModel() {
            console.log("Loading shoe model...");
            // UPDATED: Give user feedback
            loadingText.innerHTML = "Loading 3D Shoe Model...";
            const loader = new GLTFLoader();
            
            // NOTE: Make sure your file is named "shoe.glb" and uploaded
            const shoeURL = "shoe.glb"; 

            loader.load(
                shoeURL,
                function (gltf) {
                    console.log("Shoe model loaded successfully!");
                    loadingText.innerHTML = "3D Model Loaded."; // Success message
                    shoeModel = gltf.scene;
                    
                    // Start the shoe as invisible and very small
                    shoeModel.visible = false;
                    shoeModel.scale.set(0.01, 0.01, 0.01);
                    
                    // Add the shoe to our 3D world
                    scene.add(shoeModel);
                },
                undefined, // onProgress callback (optional)
                function (error) {
                    // UPDATED: Better error handling
                    console.error("Error loading shoe model: ", error);
                    loadingMessage.style.display = "block";
                    loadingText.innerHTML = `<p style="color: red;">Error: Could not load 'shoe.glb'.<br/>Please make sure the file is uploaded to GitHub and the name is correct.</p>`;
                }
            );
        }

        // --- Mission 6: 3D Render Loop ---
        function animate3D() {
            requestAnimationFrame(animate3D);
            renderer.render(scene, camera);
        }

        // --- Mission 5: Load MediaPipe Model ---
        async function createPoseLandmarker() {
            console.log("Creating Pose Landmarker...");
            // UPDATED: Give user feedback
            loadingText.innerHTML = "Loading ML Model Files...";
            try {
                const vision = await FilesetResolver.forVisionTasks(
                    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
                );
                
                loadingText.innerHTML = "Initializing ML Model... (This may take a moment)";
                
                poseLandmarker = await PoseLandmarker.createFromOptions(vision, {
                    baseOptions: {
                        modelAssetPath: `https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/pose_landmarker_lite.task`,
                        // UPDATED: Use "CPU" for maximum compatibility. 
                        // "GPU" can fail silently on some devices.
                        delegate: "CPU"
                    },
                    runningMode: runningMode,
                    numPoses: 1 // Track one person
                });
                console.log("Pose Landmarker model loaded.");
                loadingText.innerHTML = "ML Model Loaded.";

            } catch(err) {
                console.error("Error creating Pose Landmarker: ", err);
                loadingText.innerHTML = `<p style="color: red;">Error loading ML Model.<br/>${err.message}.<br/>Please reload the page.</p>`;
                throw err; // Stop the app from continuing
            }
        }

        // --- Mission 4: Start the Camera ---
        async function startCamera() {
            try {
                if (!poseLandmarker) {
                    console.log("Waiting for PoseLandmarker...");
                    loadingText.innerHTML = "Waiting for ML Model...";
                    await createPoseLandmarker(); // This now has its own try/catch
                }

                // --- Start the 3D world (Mission 6) ---
                // This will set the loading text to "Loading 3D Shoe Model..."
                setupThreeJS(); 

                // Only proceed if shoeModel is being loaded (or loaded)
                // We'll rely on the shoe loader to show an error if it fails
                
                loadingText.innerHTML = "Requesting Camera Access...";
                
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    video: { facingMode: "user" }
                });
                videoElement.srcObject = stream;

                videoElement.addEventListener("loadeddata", () => {
                    loadingMessage.style.display = "none"; // Hide loading message ONCE video starts
                    webcamRunning = true;
                    // Start the ML "Brain" loop (Mission 5 & 7)
                    predictWebcam();
                });
            } catch (err) {
                console.error("Error starting camera or loading model: ", err);
                loadingMessage.style.display = "block";
                // Check if the error is from createPoseLandmarker (which already set a message)
                if (!loadingText.innerHTML.includes("Error")) {
                     loadingText.innerHTML = `<p style="color: red;">Error: Could not start.<br/>${err.message}</p>`;
                }
            }
        }

        // --- Mission 5 & 7: The "Brain" Loop (Detect & Connect) ---
        async function predictWebcam() {
            // Set canvas size to match video
            if (videoElement.videoWidth > 0 && canvasElement.width !== videoElement.videoWidth) {
                console.log("Resizing canvas to video dimensions");
                canvasElement.width = videoElement.videoWidth;
                canvasElement.height = videoElement.videoHeight;
                // Also update 3D canvas and camera
                renderer.setSize(videoElement.videoWidth, videoElement.videoHeight);
                camera.aspect = videoElement.videoWidth / videoElement.videoHeight;
                camera.updateProjectionMatrix();
            }

            const startTimeMs = performance.now();
            if (videoElement.currentTime !== lastVideoTime) {
                lastVideoTime = videoElement.currentTime;
                
                // Run the ML model
                poseLandmarker.detectForVideo(videoElement, startTimeMs, (result) => {
                    // --- (Start of Mission 7 Code) ---

                    // Check if we have a shoe model and landmarks
                    if (shoeModel && result.landmarks && result.landmarks.length > 0) {
                        const landmarks = result.landmarks[0]; // Get landmarks for the first person

                        // Get the specific 3D landmarks for the left foot
                        // Note: These have x, y, z coordinates
                        const heel = landmarks[LANDMARK_IDS.LEFT_HEEL];
                        const toes = landmarks[LANDMARK_IDS.LEFT_FOOT_INDEX];

                        // Check if both foot points are visible
                        if (heel.visibility > 0.5 && toes.visibility > 0.5) {
                            
                            // 1. MAKE SHOE VISIBLE
                            shoeModel.visible = true;

                            // 2. CALCULATE POSITION
                            // Get the 2D midpoint on the screen
                            const footMidX = (heel.x + toes.x) / 2;
                            const footMidY = (heel.y + toes.y) / 2;
                            
                            // Convert 2D screen point to 3D world space
                            const footPos = new THREE.Vector3();
                            // MediaPipe Z is complex. We'll use a trick:
                            // We place the shoe at a fixed distance (0.9) "in front" of the camera
                            // and use the 2D x,y to find its 3D position.
                            footPos.set(
                                (footMidX * 2) - 1,   // Convert X (0-1) to (-1 to 1)
                                -(footMidY * 2) + 1,  // Convert Y (0-1) to (1 to -1)
                                0.9                   // Z-depth (0.1 = near, 1 = far)
                            );
                            
                            // This is the magic line that converts 2D screen to 3D world
                            footPos.unproject(camera);
                            
                            // Set the shoe model to this 3D position
                            shoeModel.position.copy(footPos);

                            // 3. CALCULATE ROTATION
                            // Get 2D screen angle
                            const angle = Math.atan2(
                                (toes.y - heel.y) * videoElement.videoHeight,
                                (toes.x - heel.x) * videoElement.videoWidth
                            );
                            
                            // Apply rotation (adjusting for our view)
                            // We spin it on the Z-axis (like a pinwheel)
                            shoeModel.rotation.set(0, 0, angle - Math.PI / 2);

                            // 4. CALCULATE SCALE
                            // Get 2D screen distance
                            const dx = (toes.x - heel.x) * videoElement.videoWidth;
                            const dy = (toes.y - heel.y) * videoElement.videoHeight;
                            const dist = Math.sqrt(dx*dx + dy*dy);
                            
                            // Map this distance to a 3D scale
                            // You will need to TWEAK this '0.001' number
                            // to make the shoe fit your foot perfectly.
                            const scale = dist * 0.0015; // Increased scale slightly
                            shoeModel.scale.set(scale, scale, scale);

                        } else {
                            // Foot is not visible, hide the shoe
                            shoeModel.visible = false;
                        }
                    } else {
                        // No person detected, hide the shoe
                        if (shoeModel) {
                            shoeModel.visible = false;
                        }
                    }
                    // --- (End of Mission 7 Code) ---

                    
                    // --- (Original Mission 5 Code - now hidden) ---
                    // This draws the 2D dots. We keep the code
                    // but the canvas itself is hidden by CSS.
                    canvasCtx.save();
                    canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
                    if (result.landmarks && result.landmarks.length > 0) {
                        for (const landmark of result.landmarks) {
                            drawingUtils.drawLandmarks(landmark, {
                                radius: (data) => DrawingUtils.lerp(data.from.z, -0.15, 0.1, 5, 1)
                            });
                            drawingUtils.drawConnectors(landmark, PoseLandmarker.POSE_CONNECTIONS);
                        }
                    }
                    canvasCtx.restore();
                });
            }

            // Keep the loop running
            if (webcamRunning) {
                window.requestAnimationFrame(predictWebcam);
            }
        }

        // ======================================================
        //  START THE APP
        // ======================================================
        startCamera();
    </script>
</body>
</html>

